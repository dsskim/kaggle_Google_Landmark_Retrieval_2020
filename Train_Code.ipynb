{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = [448, 448]\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n"
     ]
    }
   ],
   "source": [
    "# Detect hardware, return appropriate distribution strategy\n",
    "try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n",
    "    print('Running on TPU ', tpu.master())\n",
    "except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "else:\n",
    "    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset 관련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of train images is 3147\n"
     ]
    }
   ],
   "source": [
    "def get_paths(sub):\n",
    "    index = [\"0\",\"1\",\"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"a\",\"b\",\"c\",\"d\",\"e\",\"f\"]\n",
    "\n",
    "    paths = []\n",
    "\n",
    "    for a in index:\n",
    "        for b in index:\n",
    "            for c in index:\n",
    "                try:\n",
    "                    paths.extend([f\"../input/landmark-retrieval-2020/{sub}/{a}/{b}/{c}/\" + x for x in os.listdir(f\"../input/landmark-retrieval-2020/{sub}/{a}/{b}/{c}\")])\n",
    "                except:\n",
    "                    pass\n",
    "\n",
    "    return paths\n",
    "\n",
    "train_set_paths = get_paths('train')\n",
    "print('The number of train images is {}'.format(len(train_set_paths)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3147\n",
      "The number of unique labels is 2916\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"../input/landmark-retrieval-2020/train.csv\")\n",
    "print(len(train_df))\n",
    "train_df = train_df.sort_values(by=['landmark_id'])\n",
    "\n",
    "label_set = {}\n",
    "\n",
    "cnt = 0\n",
    "for label in train_df['landmark_id']:\n",
    "    if label not in label_set:\n",
    "        label_set[label] = cnt\n",
    "        cnt += 1\n",
    "\n",
    "print('The number of unique labels is {}'.format(len(label_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "efficientnet-b7 (Model)      (None, None, None, 2560)  64097680  \n",
      "_________________________________________________________________\n",
      "GeM (Generalized_mean_poolin (None, 2560)              1         \n",
      "_________________________________________________________________\n",
      "fc (Dense)                   (None, 512)               1311232   \n",
      "_________________________________________________________________\n",
      "batchnorm (BatchNormalizatio (None, 512)               2048      \n",
      "=================================================================\n",
      "Total params: 65,410,961\n",
      "Trainable params: 65,099,217\n",
      "Non-trainable params: 311,744\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "!pip install -q efficientnet\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "from tensorflow.keras.layers import Layer, Dense, BatchNormalization\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "class AdaCos(Layer):\n",
    "    def __init__(self, n_classes=10, regularizer=None, **kwargs):\n",
    "        super(AdaCos, self).__init__(**kwargs)\n",
    "        self.n_classes = n_classes\n",
    "        self.s = math.sqrt(2)*math.log(n_classes-1)\n",
    "        self.regularizer = regularizers.get(regularizer)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        print(input_shape)\n",
    "        print(input_shape)\n",
    "        super(AdaCos, self).build(input_shape[0])\n",
    "        self.W = self.add_weight(name='W',\n",
    "                                shape=(input_shape[0][-1], self.n_classes),\n",
    "                                initializer='glorot_uniform',\n",
    "                                trainable=True,\n",
    "                                regularizer=self.regularizer)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, y = inputs\n",
    "        # normalize feature\n",
    "        x = tf.nn.l2_normalize(x, axis=1)\n",
    "        print(x)\n",
    "        # normalize weights\n",
    "        W = tf.nn.l2_normalize(self.W, axis=0)\n",
    "        print(y)\n",
    "        # dot product\n",
    "        logits = x @ W\n",
    "        # add margin\n",
    "        # clip logits to prevent zero division when backward\n",
    "        theta = tf.acos(K.clip(logits, -1.0 + K.epsilon(), 1.0 - K.epsilon()))\n",
    "\n",
    "        B_avg = tf.where(y < 1, tf.exp(self.s*logits), tf.zeros_like(logits))\n",
    "        B_avg = tf.reduce_mean(tf.reduce_sum(B_avg, axis=1), name='B_avg')\n",
    "        theta_class = tf.gather(theta, tf.cast(y, tf.int32), name='theta_class')\n",
    "        theta_med = tfp.stats.percentile(theta_class, q=50)\n",
    "\n",
    "        with tf.control_dependencies([theta_med, B_avg]):\n",
    "            self.s = tf.math.log(B_avg) / tf.cos(tf.minimum(math.pi/4, theta_med))\n",
    "            logits = self.s * logits \n",
    "            out = tf.nn.softmax(logits)\n",
    "        return out\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.n_classes)\n",
    "\n",
    "class Generalized_mean_pooling2D(Layer):\n",
    "    def __init__(self, p=3, epsilon=1e-6, **kwargs):\n",
    "        super(Generalized_mean_pooling2D, self).__init__(**kwargs)\n",
    "\n",
    "        self.init_p = p\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        if isinstance(input_shape, list) or len(input_shape) != 4:\n",
    "            raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "        self.build_shape = input_shape\n",
    "\n",
    "        self.p = self.add_weight(\n",
    "              name='p',\n",
    "              shape=[1,],\n",
    "              initializer=tf.keras.initializers.Constant(value=self.init_p),\n",
    "              regularizer=None,\n",
    "              trainable=True,\n",
    "              dtype=tf.float32\n",
    "              )\n",
    "\n",
    "        self.built=True\n",
    "\n",
    "    def call(self, inputs):\n",
    "        input_shape = inputs.get_shape()\n",
    "        if isinstance(inputs, list) or len(input_shape) != 4:\n",
    "            raise ValueError('`GeM` pooling layer only allow 1 input with 4 dimensions(b, h, w, c)')\n",
    "\n",
    "        return (tf.reduce_mean(tf.abs(inputs**self.p), axis=[1,2], keepdims=False) + self.epsilon)**(1.0/self.p)\n",
    "\n",
    "with strategy.scope():\n",
    "    enet = efn.EfficientNetB7(\n",
    "        weights='noisy-student',\n",
    "        include_top=False\n",
    "    )\n",
    "    enet.trainable = True\n",
    "\n",
    "    embed_model = tf.keras.Sequential([\n",
    "        enet,\n",
    "        Generalized_mean_pooling2D(name='GeM'),\n",
    "        Dense(EMBEDDING_SIZE, activation='softmax', name='fc'),\n",
    "        BatchNormalization(name='batchnorm')\n",
    "    ])\n",
    "\n",
    "embed_model.compile(optimizer=tf.keras.optimizers.Adam(lr=0.0001), \n",
    "              loss = 'sparse_categorical_crossentropy',  \n",
    "              metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "\n",
    "embed_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf2] *",
   "language": "python",
   "name": "conda-env-tf2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
